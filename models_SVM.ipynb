{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 21:27:24.931082: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from data_tokenizing import tokenize_data\n",
    "from data_nlp import show_nlp_results\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/daniel.chancir/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/daniel.chancir/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/daniel.chancir/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/Users/daniel.chancir/Documents/GitHub/aprendizaje_maquinas/data_cleaning.py:45: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39665, 1000)\n",
      "(39665,)\n",
      "(9917, 1000)\n",
      "(9917,)\n",
      "1000 1000\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0   18    5    1\n",
      "   78   12    5  248    6  220 2292  508    1   13    5 1014 4812 1352\n",
      "  256  107  124   33   14   26  244   25  148  242    9 1255    1    5\n",
      "  136   64 1848  192   64 1498  656   27  575  282   35  567   78   44\n",
      "   64  656    1  124  124  108]\n",
      "49582\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "(X_train, X_test, y_train, y_test, embedding_matrix, max_len) = tokenize_data()\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(len(X_train[0]), len(X_test[0]))\n",
    "print(X_train[0])\n",
    "print(len(embedding_matrix))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(y_train)\n",
    "x_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = X_train[:-5000], X_train[-5000:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(X_train).max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x8/21809vns62d21gjxm06y07080000gp/T/ipykernel_21233/321455516.py:31: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn= create_model)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense, MaxPooling1D\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "def create_model(filters = 64, kernel_size = 3, strides=1, units = 256, \n",
    "                 optimizer='adam', rate = 0.25):\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(Embedding(len(embedding_matrix) , 100, weights=[embedding_matrix], input_length= max_len))\n",
    "    # Convolutional Layer(s)\n",
    "    model.add(Conv1D(filters = filters, kernel_size = kernel_size, strides= strides, \n",
    "                     padding='valid', activation= 'relu'))\n",
    "    model.add(MaxPooling1D(pool_size = 7))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # Dense layer(s)\n",
    "    model.add(Dense(units = units, activation= 'relu'))\n",
    "    model.add(Dropout(rate))\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation= 'sigmoid'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer= optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "# Build the model\n",
    "model = KerasClassifier(build_fn= create_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 32}\n",
      "Epoch 1/5\n",
      "542/542 [==============================] - 87s 159ms/step - loss: 0.5240 - accuracy: 0.7316 - val_loss: 0.3785 - val_accuracy: 0.8354\n",
      "Epoch 2/5\n",
      "542/542 [==============================] - 86s 159ms/step - loss: 0.3287 - accuracy: 0.8638 - val_loss: 0.3206 - val_accuracy: 0.8604\n",
      "Epoch 3/5\n",
      "542/542 [==============================] - 86s 158ms/step - loss: 0.2369 - accuracy: 0.9083 - val_loss: 0.3260 - val_accuracy: 0.8648\n",
      "Epoch 4/5\n",
      "542/542 [==============================] - 87s 160ms/step - loss: 0.1635 - accuracy: 0.9432 - val_loss: 0.3431 - val_accuracy: 0.8628\n",
      "Epoch 4: early stopping\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "{'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 128}\n",
      "Epoch 1/5\n",
      "542/542 [==============================] - 88s 161ms/step - loss: 0.4913 - accuracy: 0.7512 - val_loss: 0.3574 - val_accuracy: 0.8422\n",
      "Epoch 2/5\n",
      "542/542 [==============================] - 86s 159ms/step - loss: 0.2955 - accuracy: 0.8769 - val_loss: 0.3517 - val_accuracy: 0.8444\n",
      "Epoch 3/5\n",
      "542/542 [==============================] - 87s 161ms/step - loss: 0.2089 - accuracy: 0.9192 - val_loss: 0.3984 - val_accuracy: 0.8386\n",
      "Epoch 3: early stopping\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "{'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 512}\n",
      "Epoch 1/5\n",
      "542/542 [==============================] - 87s 159ms/step - loss: 0.4848 - accuracy: 0.7538 - val_loss: 0.3776 - val_accuracy: 0.8314\n",
      "Epoch 2/5\n",
      "542/542 [==============================] - 86s 159ms/step - loss: 0.2907 - accuracy: 0.8767 - val_loss: 0.3163 - val_accuracy: 0.8670\n",
      "Epoch 3/5\n",
      "542/542 [==============================] - 87s 160ms/step - loss: 0.1907 - accuracy: 0.9258 - val_loss: 0.3743 - val_accuracy: 0.8392\n",
      "Epoch 3: early stopping\n",
      "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
      "param_scores: [0.8628000020980835, 0.8385999798774719, 0.8392000198364258]\n",
      "best score: 0.8628000020980835\n",
      "best parameter set {'batch_size': 64, 'epochs': 5, 'filters': 128, 'kernel_size': 3, 'optimizer': 'adam', 'rate': 0.25, 'strides': 1, 'units': 32}\n"
     ]
    }
   ],
   "source": [
    "# Set the hyperparameters\n",
    "filters = [128] #[64, 128, 256]\n",
    "kernel_size = [3] #[3, 5, 7]\n",
    "strides= [1] # [1, 2, 5]\n",
    "Dense_units = [32, 128, 512]\n",
    "rate_dropouts = [0.25] #[0.1, 0.25, 0.5]\n",
    "optimizers = ['adam'] #['adam','rmsprop']\n",
    "epochs = [5]\n",
    "batches = [64] #[32, 64, 128]\n",
    "# ----------------------------------------------\n",
    "# Exhaustive Grid Search\n",
    "param_grid = dict(optimizer= optimizers, epochs= epochs, batch_size= batches,\n",
    "                  filters = filters, kernel_size = kernel_size, strides = strides, \n",
    "                  units = Dense_units, rate = rate_dropouts)\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "param_sets = list(grid)\n",
    "\n",
    "param_scores = []\n",
    "for params in grid:\n",
    "\n",
    "    print(params)\n",
    "    model.set_params(**params)\n",
    "\n",
    "    earlystopper = EarlyStopping(monitor='val_accuracy', patience= 0, verbose=1)\n",
    "    \n",
    "    history = model.fit(X_train, y_train,\n",
    "                        shuffle= True,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks= [earlystopper])\n",
    "\n",
    "    param_score = history.history['val_accuracy']\n",
    "    param_scores.append(param_score[-1])\n",
    "    print('+-'*50) \n",
    "\n",
    "p = np.argmax(np.array(param_scores))\n",
    "print('param_scores:', param_scores)\n",
    "print(\"best score:\", param_scores[p])\n",
    "# Choose best parameters\n",
    "best_params = param_sets[p]\n",
    "print(\"best parameter set\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "542/542 [==============================] - 84s 154ms/step - loss: 0.5297 - accuracy: 0.7269\n",
      "Epoch 2/5\n",
      "542/542 [==============================] - 83s 154ms/step - loss: 0.3306 - accuracy: 0.8621\n",
      "Epoch 3/5\n",
      "542/542 [==============================] - 381s 704ms/step - loss: 0.2388 - accuracy: 0.9087\n",
      "Epoch 4/5\n",
      "542/542 [==============================] - 83s 154ms/step - loss: 0.1627 - accuracy: 0.9420\n",
      "Epoch 5/5\n",
      "542/542 [==============================] - 157s 290ms/step - loss: 0.1006 - accuracy: 0.9683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fca69aa7a60>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_params(**best_params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310/310 [==============================] - 5s 15ms/step\n",
      "Test accuracy = 85.479480%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy = %f%%\" % (accuracy_score(y_test, model.predict(X_test))*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
