{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = pd.read_csv('./IMDB_reviews_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se separan la variable a predecir (sentiment) de las reseñas\n",
    "X = imdb_df['review']\n",
    "y = np.array(imdb_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crean los conjuntos de entrenamiento y de evaluación\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter for OOV tokens  is set to out of vocab and padding\n",
    "oov_tok = \"<OOV>\"\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "max_seq_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Tokenizer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenización\n",
    "#Técnica para partir los textos en partes más pequeñas y asignar un código único a cada una de estas\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000,oov_token=oov_tok)\n",
    "\n",
    "# Generate the word index dictionary\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "\n",
    "# Generate and pad the training sequences\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_seq_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [review.split() for review in X]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=8,sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.04827379 -0.06985364  0.16005489  0.29827058 -0.09532056 -0.09384312\n",
      "  0.44769517  0.03625048 -0.35462138 -0.11501411  0.10521481  0.06062239\n",
      "  0.20488    -0.04550808 -0.25560886 -0.14145426  0.05348225 -0.08228648\n",
      " -0.4239182  -0.57626235  0.24634247 -0.3239937  -0.03933471 -0.27822173\n",
      " -0.22978428 -0.10450473 -0.30146003  0.1437652   0.4607057  -0.00098847\n",
      "  0.2425458   0.23770556 -0.06206258 -0.22123651 -0.5344737   0.48707914\n",
      "  0.14440753 -0.05623389  0.08839573 -0.04574089  0.3391397  -0.52163273\n",
      " -0.07640072  0.31453443 -0.02103771 -0.43906474 -0.08852153 -0.20163462\n",
      "  0.09329267  0.41993594  0.06478628 -0.2502373   0.09069575  0.16391395\n",
      " -0.5702982   0.34577227 -0.23715968 -0.17989661 -0.22709465  0.06151568\n",
      " -0.00814888 -0.06365754  0.22962062  0.13933055 -0.09162042  0.08809141\n",
      "  0.18052064  0.16083223 -0.50842947 -0.21516995 -0.2472593   0.28001684\n",
      " -0.15539628  0.4842375   0.705848    0.19922784 -0.20424418  0.44850245\n",
      "  0.08992673  0.00338358 -0.27608138 -0.06852435 -0.181371    0.46452308\n",
      "  0.20863071  0.15428936  0.20540735  0.1835581  -0.0092136  -0.08208264\n",
      "  0.04094561  0.11716952  0.09479882  0.23011486  0.08316696 -0.10808487\n",
      "  0.20646802 -0.03554709  0.03957419 -0.21178237]\n"
     ]
    }
   ],
   "source": [
    "# Generate word embeddings from the loaded model\n",
    "vectors = []\n",
    "for sentence in sentences:\n",
    "    vector = []\n",
    "    for word in sentence:\n",
    "        if word in model.wv.key_to_index:\n",
    "            vector.append(model.wv.get_vector(word))\n",
    "    if len(vector) > 0:\n",
    "        vectors.append(np.mean(vector, axis=0))\n",
    "    else:\n",
    "        vectors.append(np.zeros(model.vector_size))\n",
    "embedding_matrix = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len = max(len(seq) for seq in x_train)\n",
    "max_len"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
